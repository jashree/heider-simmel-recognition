{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heider-Simmel Action Recognition\n",
    "\n",
    "### The code below demonstrates four supervised models that predict action labels in Heider-Simmel animation data. Run this code step by step to visualize the data processing associated with each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json, numpy, pandas, os\n",
    "from keras import backend as K\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "\n",
    "pandas.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_animations(animation_files, \n",
    "                     shape_data_columns=['big_triangle_XYR','little_triangle_XYR','circle_XYR','door_XYR']):\n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    animation_files: list of string filenames, where each filename corresponds to a single animation\n",
    "    shape_data_columns: names associated with each column of data \n",
    "                        (likely no need to change default, this is just for visualization)\n",
    "    RETURNS: \n",
    "    animation_data: A pandas DataFrame where each row is the data for a single animation.\n",
    "                    The first column is the sequence of action labels for that animation, and the subsequent columns\n",
    "                    each contain a 2-D matrix for a particular shape's X,Y,R values for each animation frame.\n",
    "                    (thus the length of each matrix is equal to the number of frames in the corresponding animation)\n",
    "    \"\"\"\n",
    "    animation_data = []\n",
    "    for animation_file in animation_files:\n",
    "        with open(animation_file, 'r') as f:\n",
    "            animation = json.load(f)\n",
    "\n",
    "        labels = [frame[0] for frame in animation] # first column is sequence of action labels\n",
    "        bt_data = numpy.array([frame[1:4] for frame in animation]) # columns 2,3,4 are X, Y, R values for big triangle\n",
    "        lt_data = numpy.array([frame[4:7] for frame in animation]) # columns 5,6,7 are X, Y, R values for little triangle\n",
    "        c_data = numpy.array([frame[7:10] for frame in animation]) # columns 8,9,10 are X, Y, R values for circle\n",
    "        d_data = numpy.array([frame[10] for frame in animation])[:, None] # column 11 is R value for door\n",
    "        #pad door data with 0's so data can ultimately be processed in matrix format\n",
    "        d_data = numpy.concatenate([numpy.zeros((len(d_data), 2)), d_data], axis=-1)\n",
    "        \n",
    "        animation_data.append([labels, bt_data, lt_data, c_data, d_data])\n",
    "\n",
    "    animation_data = pandas.DataFrame(animation_data, \n",
    "                                      columns=['labels'] + shape_data_columns)\n",
    "    return animation_data\n",
    "\n",
    "def get_x_labels(raw_data, labels_to_idxs):\n",
    "    \"\"\"\n",
    "    Extract data from DataFrame into matrix format for input/output to models\n",
    "    INPUT:\n",
    "    raw_data: a pandas DataFrame of animations\n",
    "    labels_to_idxs: a dictionary that maps string animation labels to unique integers\n",
    "    RETURNS:\n",
    "    x: numpy matrix of animation data with shape (# animations, # frames in animation, # frame features (i.e. X,Y,R), # shapes)\n",
    "    labels: numpy matrix of animation labels with shape (# animations, animation length, 1)\n",
    "    \"\"\"\n",
    "    #Assume that first column of DataFrame contains action labels, and all other columns are shape data\n",
    "    x = numpy.stack([numpy.stack(animation, axis=-1) for animation in raw_data.iloc[:,1:].as_matrix()])\n",
    "    labels = transform_labels_to_idxs(raw_data.iloc[:,0], labels_to_idxs)\n",
    "    \n",
    "    return x, labels\n",
    "\n",
    "def get_animation_snapshots(x, labels, n_snapshot_frames=100):\n",
    "    \"\"\"\n",
    "    Splits animations into \"snapshots\", i.e. segments of a fixed number of frames, where all frames in a given segment \n",
    "    correspond to the same action label. The purpose is that the model will observe the data for all frames\n",
    "    in the snapshot in parallel, as if it is observing an image. Note that the segments will be overlapping, such \n",
    "    that the beginning of each segment is one frame offset to the right relative to the beginning of the previous \n",
    "    segment, e.g. segment[frame_idx:frame_idx+n_snapshot_frames], segment[frame_idx+1: frame_idx+1+n_snapshot_frames]\n",
    "    are two neighboring snapshots given in the output.\n",
    "    \n",
    "    INPUT:\n",
    "    x: list or numpy array of data for each animation with shape (# animations, animation length, # frame features, # shapes) \n",
    "    labels: list or numpy array of label index sequences for each animation, with shape (# animations, animation_length, 1)\n",
    "    n_snapshot_frames: the number of frames contained in each output snapshot\n",
    "    \n",
    "    RETURNS:\n",
    "    all_snapshots: matrix of snapshot data with shape (# animations, # snapshots, n_snapshot_frames, # frame features, # shapes)\n",
    "    all_snapshot_labels: matrix of action labels for each snapshot with shape (# animations, # snapshots, 1)\n",
    "    \n",
    "    \"\"\"\n",
    "    all_snapshots = []\n",
    "    all_snapshot_labels = []\n",
    "    for animation, animation_labels in zip(x, labels):\n",
    "        assert(len(animation.shape) <= 3)\n",
    "        snapshots = []\n",
    "        snapshot_labels = []\n",
    "        for frame_idx in range(len(animation) - n_snapshot_frames):\n",
    "            snapshot = animation[frame_idx:frame_idx + n_snapshot_frames]\n",
    "            snapshots.append(snapshot)\n",
    "            snapshot_labels.append(animation_labels[frame_idx])\n",
    "        assert(len(snapshots) == len(snapshot_labels))\n",
    "        all_snapshots.append(numpy.array(snapshots))\n",
    "        all_snapshot_labels.append(numpy.array(snapshot_labels))\n",
    "    all_snapshots = numpy.array(all_snapshots)\n",
    "    all_snapshot_labels = numpy.array(all_snapshot_labels)\n",
    "    assert(len(all_snapshots) == len(all_snapshot_labels))\n",
    "    return all_snapshots, all_snapshot_labels\n",
    "\n",
    "def split_animations(x, labels, n_segment_frames=100):\n",
    "    \"\"\"\n",
    "    Splits animations into segments of n_segment_frames to be provided an input to an RNN model. The model will iteratate\n",
    "    through all frames in a given animation segment, so that the prediction for the action label at \n",
    "    animation[frame_idx] is conditioned upon animation[frame_idx - n_segment_frames:frame_idx]. This function is different from\n",
    "    get_animation_snapshots(), which converts each frame of the animation into a 2-D image of frames by taking \n",
    "    into account the frames that appear after it in some window. Here, we are just splitting the animation into\n",
    "    segments of datapoints that are processed as sequentially, as an alternative to the RNN observing all datapoints in \n",
    "    a given animation, which is inefficient for longer animations. Each datapoint can be a single frame or a snapshot, \n",
    "    thus this function can be called after get_animation_snapshots().\n",
    "    INPUT:\n",
    "    x: matrix of data for each animation with shape (# animations, # frames, # frame features, # shapes)\n",
    "        or (# animations, # snapshots, # snapshot frames, # frame features, # shapes) if data contains snapshots\n",
    "    labels: matrix of label index sequences for each animation, with shape (# animations, # frames, 1)\n",
    "            or (# animations, # snapshot frames, 1) if data contains snapshots\n",
    "    n_segment_frames: the number of frames contained in each output segment\n",
    "    RETURNS:\n",
    "    segments: matrix of animation segments with shape (# segments, n_segment_frames, #frame features, # shapes)\n",
    "              or (# segments, n_segment_frames, # snapshot frames, # frame features, # shapes) if data contains snapshots\n",
    "    segment_labels: matrix of animation labels with shape (# segments, n_segment_frames, 1)   \n",
    "    \n",
    "    \"\"\"\n",
    "    segments = []\n",
    "    segment_labels = []\n",
    "    for animation, animation_labels in zip(x, labels):\n",
    "        animation_segments = [animation[frame_idx:frame_idx+n_segment_frames] \n",
    "                                        for frame_idx in range(0, len(animation), n_segment_frames)]\n",
    "        animation_segment_labels = [animation_labels[frame_idx:frame_idx+n_segment_frames] \n",
    "                                    for frame_idx in range(0, len(animation), n_segment_frames)]\n",
    "        if len(animation_segments[-1]) < n_segment_frames: #if last segment is shorter than n_frames, pad with zeros\n",
    "            animation_segments[-1] = numpy.append(animation_segments[-1], \n",
    "                                                  numpy.zeros((n_segment_frames - len(animation_segments[-1]),) +\n",
    "                                                               animation_segments[-1].shape[1:]), axis=0)\n",
    "            animation_segment_labels[-1] = numpy.append(animation_segment_labels[-1],\n",
    "                                                        numpy.zeros((n_segment_frames - len(animation_segment_labels[-1]),) +\n",
    "                                                                     animation_segment_labels[-1].shape[1:]), axis=0)\n",
    "        segments.extend(numpy.array(animation_segments))\n",
    "        segment_labels.extend(numpy.array(animation_segment_labels))\n",
    "    segments = numpy.array(segments)\n",
    "    segment_labels = numpy.array(segment_labels)\n",
    "    return segments, segment_labels       \n",
    "            \n",
    "def get_label_idx_alignment(labels):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    labels: a pandas Series, where each item contains the sequence of action labels for a single animation\n",
    "    RETURNS:\n",
    "    labels_to_idxs: a dictionary where each label found in animation_labels is mapped to a unique integer\n",
    "    idxs_to_labels: a dictionary that reverses labels_to_idxs so labels can be looked up from their indices\n",
    "    \"\"\"\n",
    "    # Reserve index 0 for labels that are not in the training data \n",
    "    # This is necessary because animations with these labels could show up in the test set\n",
    "    labels_to_idxs = {'<UNKNOWN>': 0}\n",
    "    cur_label_idx = 1\n",
    "    for animation in labels:\n",
    "        for label in animation:\n",
    "            if label not in labels_to_idxs:\n",
    "                labels_to_idxs[label] = cur_label_idx\n",
    "                cur_label_idx += 1\n",
    "    \n",
    "    idxs_to_labels = {idx:label for label, idx in labels_to_idxs.items()}\n",
    "                \n",
    "    return labels_to_idxs, idxs_to_labels\n",
    "\n",
    "def transform_labels_to_idxs(labels, labels_to_idxs):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    labels: a pandas Series, where each item contains the sequence of action labels for a single animation\n",
    "    labels_to_idxs: a dictionary that maps string animation labels to unique integers\n",
    "    RETURNS:\n",
    "    labels: a matrix with shape (# animations, animation length, 1), where the final dimension contains \n",
    "    the numeric (index) representation of each action label\n",
    "    \"\"\"\n",
    "    \n",
    "    #If label is not in labels_to_idxs, assign it a label index of 0\n",
    "    labels = numpy.stack(labels.apply(lambda animation_labels: numpy.array([labels_to_idxs[label] \n",
    "                                      if label in labels_to_idxs else 0\n",
    "                                      for label in animation_labels])[:,None]).as_matrix())\n",
    "    return labels \n",
    "\n",
    "def create_mlp_model(n_snapshot_frames, n_frame_features, n_shapes, \n",
    "                     n_labels, n_hidden_layers=1, n_hidden_dim=500):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    n_snapshot_frames: # of animation frames in each snapshot\n",
    "    n_frame_features: # of input dimensions for each shape. This will likely be 3 (X,Y,R)\n",
    "    n_shapes: # of shapes in the input data. This will likely be 4 (big triangle, little triangle, circle, door)\n",
    "    n_labels: # of unique action labels in the output\n",
    "    n_hidden_layers: # of hidden layers, which can be freely tuned\n",
    "    n_hidden_dim: # of nodes in the hidden layer, which can be freely tuned\n",
    "    RETURNS:\n",
    "    model: a Keras model that is ready to be trained\n",
    "    \"\"\"\n",
    "\n",
    "    input_layer = Input(shape=(n_snapshot_frames, n_frame_features, n_shapes), name='input')\n",
    "    flatten_layer = Flatten(name='flatten')(input_layer)\n",
    "    hidden_layer = Dense(units=n_hidden_dim, activation='sigmoid', name='hidden1')(flatten_layer)\n",
    "    for layer_idx in range(1, n_hidden_layers):\n",
    "        hidden_layer = Dense(units=n_hidden_dim, activation='sigmoid', \n",
    "                             name='hidden' + str(layer_idx + 1))(hidden_layer)\n",
    "    output_layer = Dense(units=n_labels, activation='softmax', name='output')(hidden_layer)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy')\n",
    "    return model \n",
    "\n",
    "def create_rnn_model(segment_length, n_frame_features, n_shapes, \n",
    "                     n_labels, n_hidden_layers=1, n_hidden_dim=500):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    segment_length: # of frames in each animation segment given as input\n",
    "    n_frame_features: # of input dimensions for each shape. This will likely be 3 (X,Y,R)\n",
    "    n_shapes: # of shapes in the input data. This will likely be 4 (big triangle, little triangle, circle, door)\n",
    "    n_labels: # of unique action labels in the output\n",
    "    n_hidden_layers: # of hidden layers, which can be freely tuned\n",
    "    n_hidden_dim: # of nodes in the hidden layer, which can be freely tuned\n",
    "    RETURNS:\n",
    "    model: a Keras model that is ready to be trained\n",
    "    \"\"\"\n",
    "    input_layer = Input(shape=(segment_length, n_frame_features, n_shapes), name='input')\n",
    "    flatten_layer = TimeDistributed(Flatten(name='flatten'))(input_layer)\n",
    "    rnn_layer = GRU(units=n_hidden_dim, name='rnn1', return_sequences=True)(flatten_layer)\n",
    "    for layer_idx in range(1, n_hidden_layers):\n",
    "        rnn_layer = GRU(units=n_hidden_dim, name='rnn' + str(layer_idx + 1), return_sequences=True)(rnn_layer)\n",
    "    output_layer = Dense(units=n_labels, activation='softmax', name='output')(rnn_layer)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy')\n",
    "    return model\n",
    "\n",
    "def create_cnn_model(n_snapshot_frames, n_frame_features, n_shapes, n_labels, filters=5, \n",
    "                     kernel_size=10, strides=5, pool_size=2, n_hidden_layers=1, n_hidden_dim=500):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    n_snapshot_frames: # of animation frames in each snapshot\n",
    "    n_frame_features: # of input dimensions for each shape. This will likely be 3 (X,Y,R)\n",
    "    n_shapes: # of shapes in the input data. This will likely be 4 (big triangle, little triangle, circle, door)\n",
    "    n_labels: # of unique action labels in the output\n",
    "    filters: # of filters in the convolutional layer\n",
    "    kernel_size: size of kernel in the convolutional layer\n",
    "    strides: stride length in convolutional layer\n",
    "    pool_size: size of max pooling window\n",
    "    n_hidden_layers: # of hidden layers, which can be freely tuned\n",
    "    n_hidden_dim: # of nodes in the hidden layer, which can be freely tuned\n",
    "    RETURNS:\n",
    "    model: a Keras model that is ready to be trained\n",
    "    \"\"\"\n",
    "    input_layer = Input(shape=(n_snapshot_frames, n_frame_features, n_shapes), name='input')\n",
    "    # reshape to flatten n_frame_features and n_shapes into same dimension\n",
    "    reshape_layer = Reshape((n_snapshot_frames, -1))(input_layer)\n",
    "    conv_layer = Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, \n",
    "                        activation='sigmoid', padding='same', name='convolution')(reshape_layer)\n",
    "    pool_layer = MaxPooling1D(pool_size=pool_size, padding='same', name='pool')(conv_layer)\n",
    "    flatten_layer = Flatten(name='flatten')(pool_layer)\n",
    "    hidden_layer = Dense(units=n_hidden_dim, activation='sigmoid', name='hidden1')(flatten_layer)\n",
    "    for layer_idx in range(1, n_hidden_layers):\n",
    "        hidden_layer = Dense(units=n_hidden_dim, activation='sigmoid', \n",
    "                             name='hidden' + str(layer_idx + 1))(hidden_layer)\n",
    "    output_layer = Dense(units=n_labels, activation='softmax', name='output')(hidden_layer)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy')\n",
    "    return model\n",
    "\n",
    "def create_hybrid_cnn_rnn_model(segment_length, n_snapshot_frames, n_frame_features, n_shapes,\n",
    "                                n_labels, filters=5, kernel_size=10, strides=5, pool_size=2, \n",
    "                                n_hidden_layers=1, n_hidden_dim=500):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    n_snapshot_frames: # of animation frames in each snapshot\n",
    "    n_frame_features: # of input dimensions for each shape. This will likely be 3 (X,Y,R)\n",
    "    n_shapes: # of shapes in the input data. This will likely be 4 (big triangle, little triangle, circle, door)\n",
    "    n_labels: # of unique action labels in the output\n",
    "    filters: # of filters in the convolutional layer\n",
    "    kernel_size: size of kernel in the convolutional layer\n",
    "    strides: stride length in convolutional layer\n",
    "    pool_size: size of max pooling window\n",
    "    n_hidden_layers: # of hidden layers, which can be freely tuned\n",
    "    n_hidden_dim: # of nodes in the hidden layer, which can be freely tuned\n",
    "    RETURNS:\n",
    "    model: a Keras model that is ready to be trained\n",
    "    \"\"\"\n",
    "    input_layer = Input(shape=(segment_length, n_snapshot_frames, n_frame_features, n_shapes), name='input')\n",
    "    reshape_layer = Reshape((segment_length, n_snapshot_frames, -1))(input_layer)\n",
    "    conv_layer = TimeDistributed(Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, \n",
    "                                        activation='sigmoid', padding='same', name='convolution'))(reshape_layer)\n",
    "    pool_layer = TimeDistributed(MaxPooling1D(pool_size=pool_size, padding='same', name='pool'))(conv_layer)\n",
    "    flatten_layer = TimeDistributed(Flatten(name='flatten'))(pool_layer)\n",
    "    rnn_layer = GRU(units=n_hidden_dim, name='rnn1', return_sequences=True)(flatten_layer)\n",
    "    for layer_idx in range(1, n_hidden_layers):\n",
    "        rnn_layer = GRU(units=n_hidden_dim, name='rnn' + str(layer_idx + 1), return_sequences=True)(rnn_layer)\n",
    "    output_layer = Dense(units=n_labels, activation='softmax', name='output')(rnn_layer)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy')\n",
    "    return model\n",
    "\n",
    "def evaluate_prediction(model, x, labels):\n",
    "    \"\"\"\n",
    "    Computes the accuracy and perplexity of the predicted labels for x as evaluation metrics \n",
    "    (lower perplexity indicates more accurate probability predictions for correct labels).\n",
    "    \n",
    "    INPUT:\n",
    "    model: trained Keras model\n",
    "    x: matrix of animation instances where shape is same as input format to model\n",
    "    labels: matrix of animation labels where shape is same as input format to model\n",
    "    RETURNS:\n",
    "    accuracy: average accuracy of predicted labels for animations,\n",
    "    where predicted label for an animation is the one with the max probability score\n",
    "    perplexity: average perplexity of model based on predicted probability scores for labels in animations\n",
    "    \"\"\"\n",
    "    pred_probs = model.predict(x)\n",
    "    pred_probs = pred_probs.reshape((-1, pred_probs.shape[-1]))\n",
    "    labels = labels.flatten().astype(int)\n",
    "    accuracy = numpy.mean(numpy.argmax(pred_probs, axis=-1) == labels)\n",
    "    pred_probs = pred_probs[numpy.arange(len(labels)), labels]\n",
    "    perplexity = numpy.exp(-numpy.mean(numpy.log(pred_probs)))\n",
    "    return accuracy, perplexity\n",
    "\n",
    "\n",
    "def predict_labels(model, animation, n_best=1, recurrent=False):\n",
    "    \"\"\"Use the model to predict the probability of the labels for each frame in the animation.\n",
    "    Return sequences of probability scores for the n_best labels with the highest scores at each frame\n",
    "    INPUT:\n",
    "    model: trained Keras model\n",
    "    animation: a single animation instance where shape is same as input format to model\n",
    "    n_best: number of labels/probability scores to return for each frame\n",
    "    recurrent: whether or not the data is in sequential form (i.e. model has an RNN layer)\n",
    "    RETURNS:\n",
    "    all_best_labels: matrix of label predictions with shape (# frames, n_best)\n",
    "    all_best_probs: matrix of probability scores for corresponding all_best_labels with shape (# frames, n_best)\n",
    "    \"\"\"\n",
    "    all_best_labels = []\n",
    "    all_best_probs = []\n",
    "    if recurrent: #if model is RNN, append new dimension\n",
    "        pred_probs = model.predict(animation[None])[0]\n",
    "    else:\n",
    "        pred_probs = model.predict(animation)\n",
    "    for probs in pred_probs:\n",
    "        best_labels = numpy.argsort(probs)[::-1][:n_best]\n",
    "        all_best_labels.append(best_labels)\n",
    "        best_probs = probs[best_labels]\n",
    "        all_best_probs.append(best_probs)\n",
    "    all_best_labels = numpy.array(all_best_labels)\n",
    "    all_best_probs = numpy.array(all_best_probs)\n",
    "    return all_best_labels, all_best_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Load the training and testing data, put in matrix format\"\"\"\n",
    "\n",
    "\n",
    "animation_filenames = ['example_animations/' + filename for filename in os.listdir('example_animations')[:2]]\n",
    "n_test_animations = 1\n",
    "raw_train_data = parse_animations(animation_filenames[:-n_test_animations])\n",
    "raw_test_data = parse_animations(animation_filenames[-n_test_animations:])\n",
    "#Translate between string action labels and numerical indices\n",
    "labels_to_idxs, idxs_to_labels = get_label_idx_alignment(raw_train_data['labels'])\n",
    "train_x, train_labels = get_x_labels(raw_train_data, labels_to_idxs)\n",
    "test_x, test_labels = get_x_labels(raw_test_data, labels_to_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_to_idxs, idxs_to_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron (MLP)\n",
    "\n",
    "### The simplest model is a multilayer perceptron trained to predict action labels from a window of frame data compressed into a single image (a \"snapshot\"). Here, each input instance is a single snapshot. The trajectory (X,Y,R) values for each shape within the snapshot are concatenated into a single vector, and this representation is modeled by a hidden layer. The output is a probability distribution over action labels for each snapshot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Split training animations so that each input instance to the model contains \n",
    "data for multiple frames of length n_snapshot_frames. The value for n_snapshot_frames can be freely tuned.\n",
    "Each of these \"snapshots\" will correspond to a single label.\"\"\"\n",
    "\n",
    "n_snapshot_frames = 500\n",
    "train_x, train_labels = get_animation_snapshots(train_x, train_labels, n_snapshot_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"As input to model, flatten data so that the first dimension corresponds to each snapshot instead of each animation.\n",
    "In the MLP and CNN models, each label prediction is based on only on a single snapshot instead of a series.\"\"\"\n",
    "\n",
    "train_x = train_x.reshape(-1, train_x.shape[-3], train_x.shape[-2], train_x.shape[-1])\n",
    "train_labels = train_labels.reshape(-1, train_labels.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Create MLP model\"\"\"\n",
    "\n",
    "model = create_mlp_model(n_snapshot_frames=train_x.shape[1],\n",
    "                         n_frame_features=train_x.shape[-2],\n",
    "                         n_shapes=train_x.shape[-1],\n",
    "                         n_labels=len(labels_to_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Train MLP model\"\"\"\n",
    "\n",
    "loss = model.fit(x=train_x, y=train_labels,\n",
    "                 epochs=10, batch_size=100, verbose=0)\n",
    "print(loss.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Divide test data into snapshots as with training animations\"\"\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_x, test_labels = get_animation_snapshots(test_x, test_labels, n_snapshot_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_x.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Evaluate trained model on test data in terms of prediction accuracy and perplexity.\"\"\"\n",
    "\n",
    "accuracy, perplexity = evaluate_prediction(model, \n",
    "                                           test_x.reshape(-1, test_x.shape[-3], test_x.shape[-2], test_x.shape[-1]),\n",
    "                                           test_labels.reshape(-1, test_labels.shape[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Predict the label sequence probabilities for an animation; \n",
    "predict_labels() takes a single animation as input. \n",
    "Use the n_best parameter to return the top N predictions for each snapshot\"\"\"\n",
    "\n",
    "test_animation = test_x[0]\n",
    "test_animation_labels = test_labels[0]\n",
    "pred_labels, pred_probs = predict_labels(model, test_animation, n_best=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print predictions with their probabilities, alongside gold label\n",
    "\n",
    "list(zip([[idxs_to_labels[label_idx] for label_idx in labels] for labels in pred_labels],\n",
    "         pred_probs,\n",
    "         [[idxs_to_labels[label_idx] for label_idx in labels] for labels in test_animation_labels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network (CNN)\n",
    "\n",
    "### An extension of the MLP model is to use a convolutional layer to compute a feature representation of the snapshots, rather than the hidden layer directly observing all of the trajectory data in a snapshot. The inputs and outputs to this model are set up the same way as in the MLP model. There are various parameters associated with the CNN (i.e. # of filters, kernel size, stride length, and the size of the max pooling for the convolutional features). I don't have much of an intuition about what these parameter settings should be, so it might require some experimental tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Reload the training and testing data in case value of n_shapshot_frames is different for CNN\n",
    "train_x, train_labels = get_x_labels(raw_train_data, labels_to_idxs)\n",
    "test_x, test_labels = get_x_labels(raw_test_data, labels_to_idxs)\n",
    "\n",
    "n_snapshot_frames = 500\n",
    "train_x, train_labels = get_animation_snapshots(train_x, train_labels, n_snapshot_frames)\n",
    "train_x = train_x.reshape(-1, train_x.shape[-3], train_x.shape[-2], train_x.shape[-1])\n",
    "train_labels = train_labels.reshape(-1, train_labels.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Create CNN model. Can additionally specify # of filters, kernel_size, stride length, pool_size, \n",
    "# of hidden layers, and # of hidden nodes as parameters to be freely tuned.\"\"\"\n",
    "\n",
    "model = create_cnn_model(n_snapshot_frames=train_x.shape[1],\n",
    "                         n_frame_features=train_x.shape[2],\n",
    "                         n_shapes=train_x.shape[-1],\n",
    "                         n_labels=len(labels_to_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Train CNN model\"\"\"\n",
    "\n",
    "loss = model.fit(x=train_x, y=train_labels,\n",
    "                 epochs=10, batch_size=100, verbose=0)\n",
    "print(loss.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Divide test data into snapshots as with training animations\"\"\"\n",
    "\n",
    "test_x, test_labels = get_animation_snapshots(test_x, test_labels, n_snapshot_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_x.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Evaluate trained model on test data\"\"\"\n",
    "\n",
    "accuracy, perplexity = evaluate_prediction(model, \n",
    "                                           test_x.reshape(-1, test_x.shape[-3], test_x.shape[-2], test_x.shape[-1]), \n",
    "                                           test_labels.reshape(-1, test_labels.shape[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Predict the label sequence probabilities for a given animation\"\"\"\n",
    "\n",
    "test_animation = test_x[0]\n",
    "test_animation_labels = test_labels[0]\n",
    "pred_labels, pred_probs = predict_labels(model, test_animation, n_best=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print predictions alongside gold label\n",
    "\n",
    "list(zip([[idxs_to_labels[label_idx] for label_idx in labels] for labels in pred_labels],\n",
    "         pred_probs,\n",
    "         [[idxs_to_labels[label_idx] for label_idx in labels] for labels in test_animation_labels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network (RNN)\n",
    "\n",
    "### For a simple RNN model, instead of using snapshots that function as images, the animations will be split into smaller segments that maintain their sequential ordering. Each frame is a timepoint in the RNN. The prediction for the label at each frame is based on the trajectory features of the shapes at that specific frame, conditioned upon the data from all previous frames in the segment given by the hidden state of the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Divide training animations into smaller segments of frames of length n_segment_frames. The value n_segment_frames\n",
    "can be freely selected. Predictions for a label at a given frame will be based on previous frames in that \n",
    "segment. This avoids the inefficiency of loading an entire animation into the recurrent layer, making the assumption\n",
    "that the sequential information up to a particular timepoint in a segment is adequate for predicting the \n",
    "animation label at that timepoint.\"\"\"\n",
    "\n",
    "\n",
    "# Reload the datasets because format for RNN is different\n",
    "train_x, train_labels = get_x_labels(raw_train_data, labels_to_idxs)\n",
    "test_x, test_labels = get_x_labels(raw_test_data, labels_to_idxs)\n",
    "\n",
    "n_segment_frames = 1000\n",
    "train_x, train_labels = split_animations(train_x, train_labels, n_segment_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Create RNN model. Can additionally specify # of hidden layers and # of hidden dimensions \n",
    "as additional parameters to be tuned.\"\"\"\n",
    "\n",
    "model = create_rnn_model(segment_length=train_x.shape[1],\n",
    "                         n_frame_features=train_x.shape[2],\n",
    "                         n_shapes=train_x.shape[-1],\n",
    "                         n_labels=len(labels_to_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Train RNN model\"\"\"\n",
    "\n",
    "loss = model.fit(x=train_x, y=train_labels,\n",
    "                 epochs=10, batch_size=50, verbose=0)\n",
    "print(loss.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Divide test data into snapshots as with training animations\"\"\"\n",
    "\n",
    "test_x, test_labels = split_animations(test_x, test_labels, n_segment_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Evaluate trained model on test data\"\"\"\n",
    "\n",
    "accuracy, perplexity = evaluate_prediction(model, test_x, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Predict the label sequence probabilities for a given animation\"\"\"\n",
    "\n",
    "test_animation = test_x[0]\n",
    "test_animation_labels = test_labels[0]\n",
    "pred_labels, pred_probs = predict_labels(model, test_animation, n_best=1, recurrent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print predictions alongside gold label\n",
    "\n",
    "list(zip([[idxs_to_labels[label_idx] for label_idx in labels] for labels in pred_labels],\n",
    "         pred_probs,\n",
    "         [[idxs_to_labels[label_idx] for label_idx in labels] for labels in test_animation_labels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid CNN-RNN\n",
    "\n",
    "### The CNN and RNN models can be combined in a straightforward way. We extract snapshots from the animations using the procedure for the CNN, and then segment these snapshots using the procedure for the RNN. Thus, the difference between the input to the basic RNN model and this hybrid model is that here the segments consist of snapshots instead of individual frames. In the same manner as the basic CNN model above, the CNN is used to compute the features of the snapshots. Then, instead of predicting a label just from a single snapshot alone as was done for the basic CNN, the snapshots correspond to timepoints in the RNN layer. The RNN iteratively observes the CNN representation of each snapshot in the segment, encoding each into its hidden state just as it did when the timepoints were individual frames. Then the model's prediction for the label of a given snapshot is conditioned upon all previous snapshots in the segment. The motivation is that both the snapshot (short-term view) and sequential information (long-term view) are helpful for predicting action labels, and combining both signals could improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Organize animation data into snapshots as done above for CNN model. \n",
    "Then split the animations into segments as done above for the RNN model (where each segment here is a series\n",
    "of snapshots)\"\"\"\n",
    "\n",
    "# Reload the datasets\n",
    "train_x, train_labels = get_x_labels(raw_train_data, labels_to_idxs)\n",
    "test_x, test_labels = get_x_labels(raw_test_data, labels_to_idxs)\n",
    "\n",
    "n_snapshot_frames = 100\n",
    "n_segment_frames = 1000\n",
    "train_x, train_labels = get_animation_snapshots(train_x, train_labels, n_snapshot_frames)\n",
    "train_x, train_labels = split_animations(train_x, train_labels, n_segment_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Create hybrid CNN-RNN model\"\"\"\n",
    "\n",
    "model = create_hybrid_cnn_rnn_model(segment_length=train_x.shape[1],\n",
    "                                    n_snapshot_frames=train_x.shape[2],\n",
    "                                    n_frame_features=train_x.shape[-2],\n",
    "                                    n_shapes=train_x.shape[-1],\n",
    "                                    n_labels=len(labels_to_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Train hybrid model\"\"\"\n",
    "\n",
    "loss = model.fit(x=train_x, y=train_labels,\n",
    "                 epochs=10, batch_size=50, verbose=0)\n",
    "print(loss.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Divide test data into snapshots and segments as with training animations\"\"\"\n",
    "\n",
    "test_x, test_labels = get_animation_snapshots(test_x, test_labels, n_snapshot_frames)\n",
    "test_x, test_labels = split_animations(test_x, test_labels, n_segment_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Evaluate trained model on test data\"\"\"\n",
    "\n",
    "accuracy, perplexity = evaluate_prediction(model, test_x, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Predict the label sequence probabilities for a given animation\"\"\"\n",
    "\n",
    "test_animation = test_x[0]\n",
    "test_animation_labels = test_labels[0]\n",
    "pred_labels, pred_probs = predict_labels(model, test_animation, n_best=1, recurrent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print predictions alongside gold label\n",
    "\n",
    "list(zip([[idxs_to_labels[label_idx] for label_idx in labels] for labels in pred_labels],\n",
    "         pred_probs,\n",
    "         [[idxs_to_labels[label_idx] for label_idx in labels] for labels in test_animation_labels]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
